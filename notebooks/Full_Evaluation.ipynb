{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30843589-ad64-4590-8f1d-a1c43e19ba78",
   "metadata": {},
   "source": [
    "## Task 1 Full evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1883d6d3-5632-4058-af91-e62bdf36d160",
   "metadata": {},
   "source": [
    "## Task Overview\n",
    "\n",
    "# 1. Data Collection\n",
    "To build this classifier, a raw labeled dataset of grocery items and their corresponding categories was required.\n",
    "\n",
    "Data Needed: A collection of common grocery items (e.g., \"apples\", \"bananas\") paired with specific supermarket categories.\n",
    "\n",
    "Category Schema: The project utilizes nine key categories:\n",
    "\n",
    "Fruit and Veg\n",
    "\n",
    "Eggs and Dairy\n",
    "\n",
    "Bakery\n",
    "\n",
    "Pantry\n",
    "\n",
    "Snacks\n",
    "\n",
    "Household\n",
    "\n",
    "Meat and Fish\n",
    "\n",
    "Drinks\n",
    "\n",
    "Frozen Foods\n",
    "\n",
    "Rationale: These categories were selected because they are standard in supermarket organization and align with existing consumer grocery applications.\n",
    "\n",
    "Quantity: The current dataset consists of approximately 50 labeled examples to establish the initial logic.\n",
    "\n",
    "# 2. Data Processing\n",
    "Raw text data must be converted into a numerical format that a machine learning model can understand.\n",
    "\n",
    "Cleaning and Tokenization: Using the NLTK library, the item names are tokenized (split into words) and lemmatized (converted to their base form, like \"apples\" to \"apple\") to ensure consistency.\n",
    "\n",
    "Vectorization: A CountVectorizer is used to convert the cleaned text into a matrix of token counts (BoW - Bag of Words).\n",
    "\n",
    "Label Encoding: The text categories (eg \"Fruit & Vegetables\") are converted into numerical IDs (e.g., category '4') using LabelEncoder to serve as targets for the model.\n",
    "\n",
    "# 3. Machine Learning Technique\n",
    "A Neural Network built with TensorFlow/Keras was chosen for the classification task.\n",
    "\n",
    "Reason for Choice: While simpler models like Naive Bayes could work, a neural network was selected to allow for more complex feature extraction as the dataset grows. It provides a flexible framework for handling text classification through multi-class categorical cross-entropy.\n",
    "\n",
    "Improvements Considered: To improve accuracy, TF-IDF (Term Frequency-Inverse Document Frequency) was identified as a better alternative to CountVectorizer because it weights words by their relative importance, helping distinguish between items like \"milk\" and \"oat milk\".\n",
    "\n",
    "# 4. Results and Analysis\n",
    "Description of Results: Initial testing shows the model can predict categories for simple inputs, though it is limited by the small size of the training data (50 examples).\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Data Constraints: With only 50 examples, the model may struggle with \"unseen\" words or variations it hasn't encountered in the training set.\n",
    "\n",
    "Technique Limitations: The current use of CountVectorizer only counts word occurrences and doesn't account for word importance or context. Implementing n-grams was attempted to help the model recognize multi-word items, though initial results didn't meet expectations, indicating a need for more diverse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d99f9c-4834-4165-9248-e128699310b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "376975a1-04f4-4ae0-b5e5-87551e64f5a4",
   "metadata": {},
   "source": [
    "# Task 2 Full evaluation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953d62db-ca57-4914-81da-60bc7cc3c8fa",
   "metadata": {},
   "source": [
    "In this task, I developed a Market Basket Analysis (MBA) system designed to recommend complementary grocery items based on the contents of a customer's shopping basket. My approach focused on modular data handling, interpretable statistical modeling, and robust fallback logic to ensure utility even for unique inputs.\n",
    "\n",
    "# 1. Data Collection\n",
    "To train a model on realistic shopping behavior, a large-scale transactional dataset was required.\n",
    "\n",
    "Dataset Used: The UCI \"Groceries\" dataset, which contains approximately 15,000 anonymized shopping transactions.\n",
    "\n",
    "Structure: Each transaction represents a single shopping trip, recorded as a set of items purchased together.\n",
    "\n",
    "Rationale: Unlike the hand-labeled data in Task 1, this public dataset provides the volume and variety needed to extract statistically significant purchasing patterns.\n",
    "\n",
    "# 2. Data Processing\n",
    "The raw transaction data must be transformed into a format suitable for association rule mining.\n",
    "\n",
    "Basket Parsing: The raw CSV data (containing customer IDs and dates) is grouped and cleaned to create a basket_list. Each entry in this list is a sub-list of items belonging to one transaction.\n",
    "\n",
    "One-Hot Encoding: Using TransactionEncoder from the mlxtend library, the item lists are converted into a sparse matrix where columns represent items and rows represent transactions. A \"True\" value indicates an item's presence in a specific basket.\n",
    "\n",
    "Filtering: A minimum support threshold (e.g., min_support=0.005) is applied to focus on itemsets that appear frequently enough to provide reliable recommendations while still allowing for some rare item combinations.\n",
    "\n",
    "# 3. Machine Learning Technique\n",
    "The project employs the Apriori Algorithm and Association Rule Mining.\n",
    "\n",
    "Reason for Choice: Apriori is the industry standard for Market Basket Analysis because it efficiently identifies frequent itemsets. It generates \"rules\" (e.g., {pasta, olive oil} to {canned tomato}) based on three key metrics:\n",
    "\n",
    "Support: How often the itemset appears in the total dataset.\n",
    "\n",
    "Confidence: How often the recommendation is true when the initial items are present.\n",
    "\n",
    "Lift: The strength of the association (a lift > 1 indicates the items are bought together more often than by random chance).\n",
    "\n",
    "Fallback Logic: For live recommendations, a three-step approach is used: exact matches of rules, partial matches for overlapping items, and a frequency-based fallback to ensure the user always receives a suggestion.\n",
    "\n",
    "# 4. Results and Analysis\n",
    "Description of Results: The model successfully generated 57 distinct association rules from the dataset. For common items like \"whole milk\" or \"yogurt,\" the system provides high-confidence suggestions based on thousands of real-world transactions.\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Data Influence: The specific recommendations are heavily influenced by the UCI dataset's demographics. For example, \"whole milk\" appears in many rules because it is the most frequent item in the dataset.\n",
    "\n",
    "Integration with Task 1: By reusing the structured item names from Task 1, the system ensures that categorized items can be directly mapped to recommendation rules, creating a seamless user experience from item entry to suggestion.\n",
    "\n",
    "# 5. Future Improvements\n",
    "To move beyond simple co-occurrence, the system could be enhanced with Collaborative Filtering. This would allow the model to suggest items based on what similar customers bought, rather than just what is currently in the basket, providing a more personalized shopping experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262527dc-b8ca-4985-bbfc-642885d10dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56320024-a4de-45a4-af47-566e90c73d92",
   "metadata": {},
   "source": [
    "# Task 3 Full Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649ad50c-a719-4bcc-b9f0-da2d66ba85ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee763bb-b1a1-463d-9221-6741a1e71fbb",
   "metadata": {},
   "source": [
    "Building on the classification and recommendation logic established in previous steps, Task 3 introduces an Image Recognition Model using a Convolutional Neural Network (CNN). This model identifies specific grocery items from images and feeds them into the Task 1 classifier for automatic categorization.\n",
    "\n",
    "# 1. Data Collection\n",
    "To train a model capable of visual recognition, a diverse set of images was required for each grocery item.\n",
    "\n",
    "Source: Images were programmatically collected from Wikimedia Commons and Pixabay using a custom scraper.\n",
    "\n",
    "Items Selected: The model focuses on four common items across two categories:\n",
    "\n",
    "Fruit: Apples, Bananas\n",
    "\n",
    "Vegetables: Carrots, Tomatoes\n",
    "\n",
    "Quantity: Approximately 50 high-quality images were scraped per item to ensure a balanced dataset.\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "Raw images vary in size, format, and quality, requiring standardization before they can be used for training.\n",
    "\n",
    "Cleaning: A dedicated script was used to filter and remove any corrupted or unreadable image files from the dataset.\n",
    "\n",
    "Resizing & Normalization: All images are resized to a uniform 128x128 pixels. Pixel values are normalized to a range of [0, 1] by dividing by 255, which helps the neural network converge faster during training.\n",
    "\n",
    "Batching: Data is organized into batches of 32 for efficient processing during the training phase.\n",
    "\n",
    "# 3. Machine Learning Technique\n",
    "A Convolutional Neural Network (CNN) was designed specifically for this multi-class image classification task.\n",
    "\n",
    "Model Architecture: The model consists of several layers designed to extract visual features:\n",
    "\n",
    "Convolutional Layers: Three layers with increasing filters (32, 64, and 128) to detect shapes and patterns.\n",
    "\n",
    "MaxPooling Layers: Used after each convolution to reduce spatial dimensions and focus on the most important features.\n",
    "\n",
    "Dropout Layer: A 50% dropout rate is applied before the final layer to prevent overfitting, ensuring the model generalizes well to new, unseen images.\n",
    "\n",
    "Reason for Choice: CNNs are the gold standard for image recognition because they can automatically learn spatial hierarchies of features, making them far more effective than traditional flat neural networks for visual data.\n",
    "\n",
    "# 4. Results and Analysis\n",
    "Performance: The model demonstrates high accuracy in identifying \"Apples\" and \"Bananas\" on simple backgrounds.\n",
    "\n",
    "Integration with Task 1: When an image (eg an apple) is correctly identified, the string \"apple\" is passed to the Task 1 model, which then successfully assigns it to the \"Fruit and Veg\" category.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "Color Bias: Initial testing showed the model might rely too heavily on color; for instance, it once misclassified a multicolored apple as a banana.\n",
    "\n",
    "Complex Backgrounds: Items photographed in messy environments or in varied lighting conditions are harder for the current model to identify accurately.\n",
    "\n",
    "# 5. Future Improvements\n",
    "Data Augmentation: Implementing techniques like rotation, flipping, and zooming on existing images would help the model recognize items from different angles and in various lighting.\n",
    "\n",
    "Transfer Learning: Utilizing pre-trained models like ResNet or EfficientNet could significantly boost accuracy by leveraging knowledge from millions of existing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920ea122-2457-4636-8e06-51d3b77f1fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
