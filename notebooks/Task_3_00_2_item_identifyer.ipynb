{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de84aef-fc32-4c3f-8396-8db031f70ef7",
   "metadata": {},
   "source": [
    "## Load and Inspect Apple Image Dataset\n",
    "Step 1: Define the Goal\n",
    "\n",
    "In this step, we load a small, controlled image dataset containing only apples.\n",
    "The aim is to:\n",
    "\n",
    "Verify the dataset structure\n",
    "\n",
    "Ensure Keras can infer labels correctly\n",
    "\n",
    "Prepare the pipeline for incremental expansion (more fruits, more categories)\n",
    "\n",
    "At this stage, no model is trained yet.\n",
    "\n",
    "## Setup and Prepare Dataset\n",
    "\n",
    "We will train a CNN to classify **Apple vs Banana** using Keras. First, we need to load the images from the directories and normalize them for training. Each subfolder represents a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843393af-aa23-4fcd-9e95-bb07f56e0a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports and setup\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "# Dataset paths\n",
    "DATA_DIR = r\"C:\\coding5final\\coding5\\data\\images\\fruit_recognition\"\n",
    "IMAGE_SIZE = (150, 150)\n",
    "BATCH_SIZE = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea668f22-b182-4e87-98e0-d4c6358e67ca",
   "metadata": {},
   "source": [
    "# Step 2: Load and Normalize Images\n",
    "\n",
    "Use `image_dataset_from_directory` to load images automatically. Keras will infer the labels from folder names. We will normalize the images to the `[0, 1]` range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e3055f1-40af-4263-8cea-abff9f0209b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 files belonging to 2 classes.\n",
      "Using 160 files for training.\n",
      "Found 200 files belonging to 2 classes.\n",
      "Using 40 files for validation.\n",
      "Classes: ['apple', 'bannana'], Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load datasets\n",
    "train_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",  # categorical for multi-class\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Save class names and number of classes\n",
    "class_names = train_dataset.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print(f\"Classes: {class_names}, Number of classes: {NUM_CLASSES}\")\n",
    "\n",
    "# Normalize datasets\n",
    "normalization_layer = keras.layers.Rescaling(1.0 / 255)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "validation_dataset = validation_dataset.map(lambda x, y: (normalization_layer(x), y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed46a55-9d17-4d39-9c4a-3d401573acb8",
   "metadata": {},
   "source": [
    "# Step 3: Define the CNN Model\n",
    "\n",
    "We define a simple CNN for 2-class classification. The network consists of convolutional layers, max-pooling layers, dropout for regularization, and a dense softmax output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba5db3d6-702a-485a-beb8-d23dd38faef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,986</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │          \u001b[38;5;34m73,986\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167,234</span> (653.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m167,234\u001b[0m (653.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">167,234</span> (653.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m167,234\u001b[0m (653.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3: Define CNN\n",
    "IMAGE_SHAPE = (150, 150, 3)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=IMAGE_SHAPE),\n",
    "    \n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    \n",
    "    keras.layers.Dense(NUM_CLASSES, activation='softmax')  # softmax for multi-class\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4bcdd1-1355-4d70-922c-645d6c81dc30",
   "metadata": {},
   "source": [
    "# Step 4: Train the CNN Model\n",
    "\n",
    "We train the model using the Apple + Banana dataset. Validation accuracy will give insight into how well the model generalizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2969f3fd-d1d7-4f4d-ab0a-61d0a019e80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.6750 - loss: 0.5788 - val_accuracy: 0.8750 - val_loss: 0.3214\n",
      "Epoch 2/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9187 - loss: 0.1865 - val_accuracy: 0.9500 - val_loss: 0.1465\n",
      "Epoch 3/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.9688 - loss: 0.0755 - val_accuracy: 1.0000 - val_loss: 0.0424\n",
      "Epoch 4/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - accuracy: 0.9937 - loss: 0.0231 - val_accuracy: 1.0000 - val_loss: 0.0279\n",
      "Epoch 5/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.0125 - val_accuracy: 1.0000 - val_loss: 0.0124\n",
      "Epoch 6/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 0.0090 - val_accuracy: 1.0000 - val_loss: 0.0059\n",
      "Epoch 7/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 1.0000 - val_loss: 0.0040\n",
      "Epoch 8/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 5.9661e-04 - val_accuracy: 1.0000 - val_loss: 0.0016\n",
      "Epoch 9/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 3.2194e-04 - val_accuracy: 1.0000 - val_loss: 5.1000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 2.1998e-04 - val_accuracy: 1.0000 - val_loss: 4.5145e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 1.0000 - loss: 1.1736e-04 - val_accuracy: 1.0000 - val_loss: 4.2040e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - accuracy: 1.0000 - loss: 1.2872e-04 - val_accuracy: 1.0000 - val_loss: 4.0157e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 1.0253e-04 - val_accuracy: 1.0000 - val_loss: 5.7777e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 1.0000 - loss: 6.9821e-05 - val_accuracy: 1.0000 - val_loss: 3.4858e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - accuracy: 1.0000 - loss: 5.1930e-05 - val_accuracy: 1.0000 - val_loss: 3.7410e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 3.7410e-04\n",
      "Validation Accuracy: 100.00%\n",
      "Trained model saved at: C:\\coding5final\\coding5\\data\\processed\\apple_banana_cnn_model.keras\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Train the model\n",
    "EPOCHS = 15\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "val_loss, val_accuracy = model.evaluate(validation_dataset)\n",
    "print(f\"Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = r\"C:\\coding5final\\coding5\\data\\processed\\apple_banana_cnn_model.keras\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Trained model saved at: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005e319-d3e5-40b2-a991-8f45e958574b",
   "metadata": {},
   "source": [
    "## Trained on 40 images per data type \n",
    "Epoch 1/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 2s 94ms/step - accuracy: 0.5156 - loss: 0.7120 - val_accuracy: 0.5000 - val_loss: 0.6479\n",
    "Epoch 2/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 97ms/step - accuracy: 0.8750 - loss: 0.5585 - val_accuracy: 0.7500 - val_loss: 0.4844\n",
    "Epoch 3/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 84ms/step - accuracy: 0.8594 - loss: 0.3385 - val_accuracy: 1.0000 - val_loss: 0.1396\n",
    "Epoch 4/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 88ms/step - accuracy: 0.9688 - loss: 0.1534 - val_accuracy: 1.0000 - val_loss: 0.0477\n",
    "Epoch 5/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 80ms/step - accuracy: 0.9844 - loss: 0.0710 - val_accuracy: 1.0000 - val_loss: 0.0460\n",
    "Epoch 6/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 92ms/step - accuracy: 0.9844 - loss: 0.0482 - val_accuracy: 1.0000 - val_loss: 0.0058\n",
    "Epoch 7/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 69ms/step - accuracy: 1.0000 - loss: 0.0103 - val_accuracy: 1.0000 - val_loss: 0.0017\n",
    "Epoch 8/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 65ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 1.0000 - val_loss: 0.0035\n",
    "Epoch 9/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
    "Epoch 10/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 94ms/step - accuracy: 1.0000 - loss: 3.3300e-04 - val_accuracy: 1.0000 - val_loss: 6.7298e-04\n",
    "Epoch 11/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 92ms/step - accuracy: 1.0000 - loss: 2.6332e-04 - val_accuracy: 1.0000 - val_loss: 4.7121e-04\n",
    "Epoch 12/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 77ms/step - accuracy: 1.0000 - loss: 1.8015e-04 - val_accuracy: 1.0000 - val_loss: 3.5924e-04\n",
    "Epoch 13/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 89ms/step - accuracy: 1.0000 - loss: 1.7802e-04 - val_accuracy: 1.0000 - val_loss: 2.7370e-04\n",
    "Epoch 14/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 67ms/step - accuracy: 1.0000 - loss: 1.7219e-04 - val_accuracy: 1.0000 - val_loss: 2.6784e-04\n",
    "Epoch 15/15\n",
    "8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 85ms/step - accuracy: 1.0000 - loss: 1.1774e-04 - val_accuracy: 1.0000 - val_loss: 3.0617e-04\n",
    "2/2 ━━━━━━━━━━━━━━━━━━━━ 0s 43ms/step - accuracy: 1.0000 - loss: 3.0617e-04\n",
    "Validation Accuracy: 100.00%\n",
    "Trained model saved at: C:\\coding5final\\coding5\\data\\processed\\apple_banana_cnn_model.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea9eadb-b94f-4fc5-bb9d-94a6652159df",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Training accuracy is hitting 100% quickly\n",
    "\n",
    "Epoch 3–4: validation accuracy is already 1.0.\n",
    "\n",
    "This happens because your dataset is very small (only >40 images per class) and simple. The model can memorize the training images.\n",
    "\n",
    "Loss dropping to almost zero\n",
    "\n",
    "Again, because the model is overfitting: it “learned” the training images perfectly.\n",
    "\n",
    "Validation accuracy = 100%\n",
    "\n",
    "This looks great, but with such a small dataset, it’s not a reliable measure, your model may fail on new/unseen images.\n",
    "\n",
    "What this means\n",
    "\n",
    "The training process is working technically, but the model is overfitting heavily, it memorizes the tiny dataset rather than learning general features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a66a76-8ce5-4ad1-8d06-18ac6d215240",
   "metadata": {},
   "source": [
    "### Dataset Expansion\n",
    "\n",
    "The dataset has now been expanded to include **100 images per class**, more than doubling the size of the training data compared to the initial setup.  \n",
    "\n",
    "This increase in data will help the model:  \n",
    "\n",
    "- Learn more robust features for each fruit type  \n",
    "- Reduce overfitting to the small initial sample  \n",
    "- Improve generalization to unseen images during inference  \n",
    "\n",
    "With more varied examples, the model should now be better equipped to classify new images accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0116a2-0db9-410c-bc18-25c86f48e246",
   "metadata": {},
   "source": [
    "### Step 5: Test Model with New Images\n",
    "\n",
    "We can input any image path and the model will predict the class along with its confidence. This allows testing on images outside the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7047b105-6f26-4cb3-aefb-c0fed397dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter image path to classify (any fruit or vegetable):  C:\\coding5final\\coding5\\data\\outsidedata\\appleredyellowgreen.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Predicted class: bannana\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Make sure IMAGE_SIZE matches what your model was trained on\n",
    "IMAGE_SIZE = (150, 150)\n",
    "\n",
    "def predict_image(model, img_path, class_names):\n",
    "    # Load and preprocess the image\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=IMAGE_SIZE)\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # make batch\n",
    "    img_array = img_array / 255.0  # normalize\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = model.predict(img_array)\n",
    "    class_index = np.argmax(predictions, axis=1)[0]\n",
    "    predicted_class = class_names[class_index]\n",
    "\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "\n",
    "# Example usage\n",
    "img_path = input(\"Enter image path to classify (any fruit or vegetable): \")\n",
    "predict_image(model, img_path, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0960b345-44c9-4a9a-b36d-15b42043bdec",
   "metadata": {},
   "source": [
    "## Bannana test\n",
    "Given a picture of a bannana on a wooden board (unlike the test data), the model was accurate.\n",
    "\n",
    "    Enter image path to classify (any fruit or vegetable):  C:\\coding5final\\coding5\\data\\outsidedata\\banana-7h4m9.webp\n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 88ms/step\n",
    "Predicted class: bannana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbe604-4a2f-4694-b3fb-08fe6dc1b6d0",
   "metadata": {},
   "source": [
    "## Apple test\n",
    "Given a picture of an apple on a white background (again, unlike the test data), model was accurate \n",
    "\n",
    "Enter image path to classify (any fruit or vegetable):  C:\\coding5final\\coding5\\data\\outsidedata\\Apple.webp\n",
    "\n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 49ms/step\n",
    "\n",
    "Predicted class: apple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662c4849-1cf8-4b67-8204-0e7c1969762c",
   "metadata": {},
   "source": [
    "## Yellow, green and red apple test\n",
    "Given a more confusing dataset, it got it wrong! Assuming it's based it on colour! \n",
    "\n",
    "\n",
    "Enter image path to classify (any fruit or vegetable):  C:\\coding5final\\coding5\\data\\outsidedata\\appleredyellowgreen.jpg\n",
    "\n",
    "1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 63ms/step\n",
    "\n",
    "Predicted class: bannana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4db08e-ea73-4303-87a3-58a61c88a917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
